<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta name="description" content="前言卷积神经网络CNN是Deep Learning的一个重要算法，在很多应用上表现出卓越的效果，中对比多重算法在文档字符识别的效果，结论是CNN优于其他所有的算法。CNN在手写体识别取得最好的效果，将CNN应用在基于人脸的性别识别，效果也非常不错。BP神经网络对手机拍照图片的数字进行识别，效果还算不错，接近98%，但在汉字识别上表现不佳，于是想试试卷积神经网络。 1、CNN的整体网络结构　　卷积神">
<meta property="og:type" content="article">
<meta property="og:title" content="卷积神经网络算法的一个实现">
<meta property="og:url" content="http://JackArch.github.io/2018/04/09/180409_卷积神经网络算法的一个实现 /index.html">
<meta property="og:site_name" content="Deep  |  Mind">
<meta property="og:description" content="前言卷积神经网络CNN是Deep Learning的一个重要算法，在很多应用上表现出卓越的效果，中对比多重算法在文档字符识别的效果，结论是CNN优于其他所有的算法。CNN在手写体识别取得最好的效果，将CNN应用在基于人脸的性别识别，效果也非常不错。BP神经网络对手机拍照图片的数字进行识别，效果还算不错，接近98%，但在汉字识别上表现不佳，于是想试试卷积神经网络。 1、CNN的整体网络结构　　卷积神">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://jackarch.github.io/2018/04/09/180409_卷积神经网络算法的一个实现%20/pic/180401.jpg">
<meta property="og:image" content="http://jackarch.github.io/2018/04/09/180409_卷积神经网络算法的一个实现%20/pic/180402.jpg">
<meta property="og:image" content="http://jackarch.github.io/2018/04/09/180409_卷积神经网络算法的一个实现%20/pic/180403.jpg">
<meta property="og:image" content="http://jackarch.github.io/2018/04/09/180409_卷积神经网络算法的一个实现%20/pic/180404.jpg">
<meta property="og:image" content="http://jackarch.github.io/2018/04/09/180409_卷积神经网络算法的一个实现%20/pic/180405.jpg">
<meta property="og:image" content="http://jackarch.github.io/2018/04/09/180409_卷积神经网络算法的一个实现%20/pic/180406.jpg">
<meta property="og:image" content="http://jackarch.github.io/2018/04/09/180409_卷积神经网络算法的一个实现%20/pic/180407.jpg">
<meta property="og:image" content="http://jackarch.github.io/2018/04/09/180409_卷积神经网络算法的一个实现%20/pic/180408.jpg">
<meta property="og:image" content="http://jackarch.github.io/2018/04/09/180409_卷积神经网络算法的一个实现%20/pic/180409.jpg">
<meta property="og:updated_time" content="2019-06-18T18:56:30.027Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="卷积神经网络算法的一个实现">
<meta name="twitter:description" content="前言卷积神经网络CNN是Deep Learning的一个重要算法，在很多应用上表现出卓越的效果，中对比多重算法在文档字符识别的效果，结论是CNN优于其他所有的算法。CNN在手写体识别取得最好的效果，将CNN应用在基于人脸的性别识别，效果也非常不错。BP神经网络对手机拍照图片的数字进行识别，效果还算不错，接近98%，但在汉字识别上表现不佳，于是想试试卷积神经网络。 1、CNN的整体网络结构　　卷积神">
<meta name="twitter:image" content="http://jackarch.github.io/2018/04/09/180409_卷积神经网络算法的一个实现%20/pic/180401.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://JackArch.github.io/2018/04/09/180409_卷积神经网络算法的一个实现 /">





  <title>卷积神经网络算法的一个实现 | Deep  |  Mind</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Deep  |  Mind</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://JackArch.github.io/2018/04/09/180409_卷积神经网络算法的一个实现 /">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="zhuangzhouzhishui">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Deep  |  Mind">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">卷积神经网络算法的一个实现</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-04-09T10:16:37+08:00">
                2018-04-09
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a><strong>前言</strong></h1><p>卷积神经网络CNN是Deep Learning的一个重要算法，在很多应用上表现出卓越的效果，中对比多重算法在文档字符识别的效果，结论是CNN优于其他所有的算法。CNN在手写体识别取得最好的效果，将CNN应用在基于人脸的性别识别，效果也非常不错。BP神经网络对手机拍照图片的数字进行识别，效果还算不错，接近98%，但在汉字识别上表现不佳，于是想试试卷积神经网络。</p>
<h1 id="1、CNN的整体网络结构"><a href="#1、CNN的整体网络结构" class="headerlink" title="1、CNN的整体网络结构"></a><strong>1、CNN的整体网络结构</strong></h1><p>　　卷积神经网络是在BP神经网络的改进，与BP类似，都采用了前向传播计算输出值，反向传播调整权重和偏置；CNN与标准的BP最大的不同是：CNN中相邻层之间的神经单元并不是全连接，而是部分连接，也就是某个神经单元的感知区域来自于上层的部分神经单元，而不是像BP那样与所有的神经单元相连接。CNN的有三个重要的思想架构：</p>
<ul>
<li>局部区域感知</li>
<li>权重共享</li>
<li>空间或时间上的采样</li>
</ul>
<p>　　局部区域感知能够发现数据的一些局部特征，比如图片上的一个角，一段弧，这些基本特征是构成动物视觉的基础；而BP中，所有的像素点是一堆混乱的点，相互之间的关系没有被挖掘。</p>
<p>　　CNN中每一层的由多个map组成，每个map由多个神经单元组成，同一个map的所有神经单元共用一个卷积核（即权重），卷积核往往代表一个特征，比如某个卷积和代表一段弧，那么把这个卷积核在整个图片上滚一下，卷积值较大的区域就很有可能是一段弧。注意卷积核其实就是权重，我们并不需要单独去计算一个卷积，而是一个固定大小的权重矩阵去图像上匹配时，这个操作与卷积类似，因此我们称为卷积神经网络，实际上，BP也可以看做一种特殊的卷积神经网络，只是这个卷积核就是某层的所有权重，即感知区域是整个图像。权重共享策略减少了需要训练的参数，使得训练出来的模型的泛华能力更强。</p>
<p>　　采样的目的主要是混淆特征的具体位置，因为某个特征找出来后，它的具体位置已经不重要了，我们只需要这个特征与其他的相对位置，比如一个“8”，当我们得到了上面一个”o”时，我们不需要知道它在图像的具体位置，只需要知道它下面又是一个“o”我们就可以知道是一个’8’了，因为图片中”8”在图片中偏左或者偏右都不影响我们认识它，这种混淆具体位置的策略能对变形和扭曲的图片进行识别。</p>
<p>　　CNN的这三个特点是其对输入数据在空间（主要针对图像数据）上和时间（主要针对时间序列数据，参考<a href="http://en.wikipedia.org/wiki/Time_delay_neural_network" target="_blank" rel="noopener">TDNN</a>）上的扭曲有很强的鲁棒性。CNN一般采用卷积层与采样层交替设置，即一层卷积层接一层采样层，采样层后接一层卷积…这样卷积层提取出特征，再进行组合形成更抽象的特征，最后形成对图片对象的描述特征，CNN后面还可以跟全连接层，全连接层跟BP一样。下面是一个卷积神经网络的示例：</p>
<p><img src="./pic/180401.jpg" alt></p>
<p>图1（<a href="https://www.cnblogs.com/fengfenggirl/p/cnn_implement.html#refren1" target="_blank" rel="noopener">图片来源</a>）</p>
<p>　　卷积神经网络的基本思想是这样，但具体实现有多重版本，我参考了matlab的Deep Learning的工具箱<a href="https://github.com/rasmusbergpalm/DeepLearnToolbox" target="_blank" rel="noopener">DeepLearnToolbox</a>，这里实现的CNN与其他最大的差别是采样层没有权重和偏置，仅仅只对卷积层进行一个采样过程，这个工具箱的测试数据集是MINIST，每张图像是28*28大小，它实现的是下面这样一个CNN：</p>
<p><img src="./pic/180402.jpg" alt></p>
<p>图2</p>
<h1 id="2、网络初始化"><a href="#2、网络初始化" class="headerlink" title="2、网络初始化"></a><strong>2、网络初始化</strong></h1><p>　　CNN的初始化主要是初始化卷积层和输出层的卷积核（权重）和偏置，<a href="https://github.com/rasmusbergpalm/DeepLearnToolbox" target="_blank" rel="noopener">DeepLearnToolbox</a>里面对卷积核和权重进行随机初始化，而对偏置进行全0初始化。</p>
<h1 id="3、前向传输计算"><a href="#3、前向传输计算" class="headerlink" title="3、前向传输计算"></a><strong>3、前向传输计算</strong></h1><p>　　前向计算时，输入层、卷积层、采样层、输出层的计算方式不相同。</p>
<p>　　<strong>3.1 输入层</strong>：输入层没有输入值，只有一个输出向量，这个向量的大小就是图片的大小，即一个28*28矩阵;</p>
<p>　　<strong>3.2 卷积层</strong>：卷积层的输入要么来源于输入层，要么来源于采样层，如上图红色部分。卷积层的每一个map都有一个大小相同的卷积核，Toolbox里面是5<em>5的卷积核。下面是一个示例，为了简单起见，卷积核大小为2</em>2，上一层的特征map大小为4<em>4，用这个卷积在图片上滚一遍，得到一个一个(4-2+1)</em>（4-2+1）=3<em>3的特征map，卷积核每次移动一步，因此。在Toolbox的实现中，卷积层的一个map与上层的所有map都关联，如上图的S2和C3，即C3共有6</em>12个卷积核，卷积层的每一个特征map是不同的卷积核在前一层所有map上作卷积并将对应元素累加后加一个偏置，再求sigmod得到的。还有需要注意的是，卷积层的map个数是在网络初始化指定的，而卷积层的map的大小是由卷积核和上一层输入map的大小决定的，假设上一层的map大小是n<em>n、卷积核的大小是k</em>k，则该层的map大小是(n-k+1)<em>(n-k+1)，比如上图的24</em>24的map大小24=（28-5+1）。 斯坦福的<a href="http://deeplearning.stanford.edu/wiki/index.php/卷积特征提取" target="_blank" rel="noopener">深度学习教程</a>更加详细的介绍了卷积特征提取的计算过程。                                                                       <img src="./pic/180403.jpg" alt></p>
<p>　　图3</p>
<p>　　<strong>3.3 采样层（subsampling,Pooling）</strong>：采样层是对上一层map的一个采样处理，这里的采样方式是对上一层map的相邻小区域进行聚合统计，区域大小为scale<em>scale，有些实现是取小区域的最大值，而ToolBox里面的实现是采用2</em>2小区域的均值。注意，卷积的计算窗口是有重叠的，而采用的计算窗口没有重叠，ToolBox里面计算采样也是用卷积(conv2(A,K,’valid’))来实现的，卷积核是2*2，每个元素都是1/4，去掉计算得到的卷积结果中有重叠的部分，即：                                                                <img src="./pic/180404.jpg" alt></p>
<p>图4</p>
<h1 id="4、反向传输调整权重"><a href="#4、反向传输调整权重" class="headerlink" title="4、反向传输调整权重"></a><strong>4、反向传输调整权重</strong></h1><p>　　反向传输过程是CNN最复杂的地方，虽然从宏观上来看基本思想跟BP一样，都是通过最小化残差来调整权重和偏置，但CNN的网络结构并不像BP那样单一，对不同的结构处理方式不一样，而且因为权重共享，使得计算残差变得很困难，很多论文<a href="https://www.cnblogs.com/fengfenggirl/p/cnn_implement.html#refren1" target="_blank" rel="noopener">[1]</a><a href="https://www.cnblogs.com/fengfenggirl/p/cnn_implement.html#ref5" target="_blank" rel="noopener">[5]</a>和文章<a href="https://www.cnblogs.com/fengfenggirl/p/cnn_implement.html#ref4" target="_blank" rel="noopener">[4]</a>都进行了详细的讲述，但我发现还是有一些细节没有讲明白，特别是采样层的残差计算，我会在这里详细讲述。</p>
<p>　　<strong>4.1输出层的残差</strong></p>
<p>　　和BP一样，CNN的输出层的残差与中间层的残差计算方式不同，输出层的残差是输出值与类标值得误差值，而中间各层的残差来源于下一层的残差的加权和。输出层的残差计算如下：</p>
<p><img src="./pic/180405.jpg" alt></p>
<p><a href="http://deeplearning.stanford.edu/wiki/index.php/反向传导算法" target="_blank" rel="noopener">公式来源</a></p>
<p>　　这个公式不做解释，可以查看公式来源，看斯坦福的深度学习教程的解释。</p>
<p>　　<strong>4.2 下一层为采样层（subsampling）的卷积层的残差</strong></p>
<p>　　当一个卷积层L的下一层(L+1)为采样层，并假设我们已经计算得到了采样层的残差，现在计算该卷积层的残差。从最上面的网络结构图我们知道，采样层（L+1）的map大小是卷积层L的1/（scale<em>scale），ToolBox里面，scale取2，但这两层的map个数是一样的，卷积层L的某个map中的4个单元与L+1层对应map的一个单元关联，可以对采样层的残差与一个scale</em>scale的全1矩阵进行<a href="http://zh.wikipedia.org/wiki/克罗内克积" target="_blank" rel="noopener">克罗内克积</a>进行扩充，使得采样层的残差的维度与上一层的输出map的维度一致，Toolbox的代码如下，其中d表示残差，a表示输出值：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net.layers&#123;l&#125;.d&#123;j&#125; = net.layers&#123;l&#125;.a&#123;j&#125; .* (1 - net.layers&#123;l&#125;.a&#123;j&#125;) .* expand(net.layers&#123;l + 1&#125;.d&#123;j&#125;, [net.layers&#123;l + 1&#125;.scale net.layers&#123;l + 1&#125;.scale 1])</span><br></pre></td></tr></table></figure>



<p>　　扩展过程：</p>
<p><img src="./pic/180406.jpg" alt></p>
<p>图5</p>
<p>　　利用卷积计算卷积层的残差：</p>
<p><img src="./pic/180407.jpg" alt></p>
<p>图6</p>
<p>　　<strong>4.3 下一层为卷积层（subsampling）的采样层的残差</strong></p>
<p>　　当某个采样层L的下一层是卷积层(L+1)，并假设我们已经计算出L+1层的残差，现在计算L层的残差。采样层到卷积层直接的连接是有权重和偏置参数的，因此不像卷积层到采样层那样简单。现再假设L层第j个map Mj与L+1层的M2j关联，按照BP的原理，L层的残差Dj是L+1层残差D2j的加权和，但是这里的困难在于，我们很难理清M2j的那些单元通过哪些权重与Mj的哪些单元关联，Toolbox里面还是采用卷积（稍作变形）巧妙的解决了这个问题，其代码为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">convn(net.layers&#123;l + 1&#125;.d&#123;j&#125;, rot180(net.layers&#123;l + 1&#125;.k&#123;i&#125;&#123;j&#125;), &apos;full&apos;);</span><br></pre></td></tr></table></figure>



<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rot180表示对矩阵进行180度旋转（可通过行对称交换和列对称交换完成），为什么这里要对卷积核进行旋转，答案是：通过这个旋转，&apos;full&apos;模式下得卷积的正好抓住了前向传输计算上层map单元与卷积和及当期层map的关联关系，需要注意的是matlab的内置函数convn在计算卷积前，会对卷积核进行一次旋转，因此我们之前的所有卷积的计算都对卷积核进行了旋转：</span><br></pre></td></tr></table></figure>



<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">a =</span><br><span class="line">     1     1     1</span><br><span class="line">     1     1     1</span><br><span class="line">     1     1     1</span><br><span class="line">k =</span><br><span class="line">     1     2     3</span><br><span class="line">     4     5     6</span><br><span class="line">     7     8     9</span><br><span class="line"></span><br><span class="line">&gt;&gt; convn(a,k,&apos;full&apos;)</span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">     1     3     6     5     3</span><br><span class="line">     5    12    21    16     9</span><br><span class="line">    12    27    45    33    18</span><br><span class="line">    11    24    39    28    15</span><br><span class="line">     7    15    24    17     9</span><br></pre></td></tr></table></figure>

<p> 　　convn在计算前还会对待卷积矩阵进行0扩展，如果卷积核为k<em>k，待卷积矩阵为n</em>n，需要以n<em>n原矩阵为中心扩展到(n+2(k-1))</em>(n+2(k-1))，所有上面convn(a,k,’full’)的计算过程如下：</p>
<p><img src="./pic/180408.jpg" alt></p>
<p>图7</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">实际上convn内部是否旋转对网络训练没有影响，只要内部保持一致（即都要么旋转，要么都不旋转），所有我的卷积实现里面没有对卷积核旋转。如果在convn计算前，先对卷积核旋转180度，然后convn内部又对其旋转180度，相当于卷积核没有变。</span><br></pre></td></tr></table></figure>



<p>　　为了描述清楚对卷积核旋转180与卷积层的残差的卷积所关联的权重与单元，正是前向计算所关联的权重与单元，我们选一个稍微大一点的卷积核，即假设卷积层采用用3<em>3的卷积核，其上一层采样层的输出map的大小是5</em>5，那么前向传输由采样层得到卷积层的过程如下：</p>
<p><img src="./pic/180409.jpg" alt></p>
<p>图8</p>
<p>　　这里我们采用自己实现的convn（即内部不会对卷积核旋转），并假定上面的矩阵A、B下标都从1开始，那么有：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">B11 = A11*K11 + A12*K12 + A13*K13 + A21*K21 + A22*K22 + A23*K23 + A31*K31 + A32*K32 + A33*K33</span><br><span class="line">B12 = A12*K11 + A13*K12 + A14*K13 + A22*K21 + A23*K22 + A24*K23 + A32*K31 + A33*K32 + A34*K33</span><br><span class="line">B13 = A13*K11 + A14*K12 + A15*K13 + A23*K21 + A24*K22 + A25*K23 + A33*K31 + A34*K32 + A35*K33</span><br><span class="line">B21 = A21*K11 + A22*K12 + A23*K13 + A31*K21 + A32*K22 + A33*K23 + A41*K31 + A42*K32 + A43*K33</span><br><span class="line">B22 = A22*K11 + A23*K12 + A24*K13 + A32*K21 + A33*K22 + A34*K23 + A42*K31 + A43*K32 + A44*K33</span><br><span class="line">B23 = A23*K11 + A24*K12 + A25*K13 + A33*K21 + A34*K22 + A35*K23 + A43*K31 + A44*K32 + A45*K33</span><br><span class="line">B31 = A31*K11 + A32*K12 + A33*K13 + A41*K21 + A42*K22 + A43*K23 + A51*K31 + A52*K32 + A53*K33</span><br><span class="line">B32 = A32*K11 + A33*K12 + A34*K13 + A42*K21 + A43*K22 + A44*K23 + A52*K31 + A53*K32 + A54*K33</span><br><span class="line">B33 = A33*K11 + A34*K12 + A35*K13 + A43*K21 + A44*K22 + A45*K23 + A53*K31 + A54*K32 + A55*K33</span><br></pre></td></tr></table></figure>

<p>　　我们可以得到B矩阵每个单元与哪些卷积核单元和哪些A矩阵的单元之间有关联：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">A11 [K11] [B11]</span><br><span class="line">A12 [K12, K11] [B12, B11]</span><br><span class="line">A13 [K13, K12, K11] [B12, B13, B11]</span><br><span class="line">A14 [K13, K12] [B12, B13]</span><br><span class="line">A15 [K13] [B13]</span><br><span class="line">A21 [K21, K11] [B21, B11]</span><br><span class="line">A22 [K22, K21, K12, K11] [B12, B22, B21, B11]</span><br><span class="line">A23 [K23, K22, K21, K13, K12, K11] [B23, B22, B21, B12, B13, B11]</span><br><span class="line">A24 [K23, K22, K13, K12] [B23, B12, B13, B22]</span><br><span class="line">A25 [K23, K13] [B23, B13]</span><br><span class="line">A31 [K31, K21, K11] [B31, B21, B11]</span><br><span class="line">A32 [K32, K31, K22, K21, K12, K11] [B31, B32, B22, B12, B21, B11]</span><br><span class="line">A33 [K33, K32, K31, K23, K22, K21, K13, K12, K11] [B23, B22, B21, B31, B12, B13, B11, B33, B32]</span><br><span class="line">A34 [K33, K32, K23, K22, K13, K12] [B23, B22, B32, B33, B12, B13]</span><br><span class="line">A35 [K33, K23, K13] [B23, B13, B33]</span><br><span class="line">A41 [K31, K21] [B31, B21]</span><br><span class="line">A42 [K32, K31, K22, K21] [B32, B22, B21, B31]</span><br><span class="line">A43 [K33, K32, K31, K23, K22, K21] [B31, B23, B22, B32, B33, B21]</span><br><span class="line">A44 [K33, K32, K23, K22] [B23, B22, B32, B33]</span><br><span class="line">A45 [K33, K23] [B23, B33]</span><br><span class="line">A51 [K31] [B31]</span><br><span class="line">A52 [K32, K31] [B31, B32]</span><br><span class="line">A53 [K33, K32, K31] [B31, B32, B33]</span><br><span class="line">A54 [K33, K32] [B32, B33]</span><br><span class="line">A55 [K33] [B33]</span><br></pre></td></tr></table></figure>

<p>　　然后再用matlab的convn(内部会对卷积核进行180度旋转)进行一次convn(B,K,’full’)，结合图7，看红色部分，除去0，A11=B’33<em>K’33=B11</em>K11，发现A11正好与K11、B11关联对不对；我们再看一个A24=B’34<em>K’21+B’35</em>K’22+B’44<em>K’31+B’45</em>K’32=B12<em>K23+B13</em>K22+B22<em>K13+B23</em>K12，发现参与A24计算的卷积核单元与B矩阵单元，正好是前向计算时关联的单元，所以我们可以通过旋转卷积核后进行卷积而得到采样层的残差。</p>
<p> 　　残差计算出来后，剩下的就是用更新权重和偏置，这和BP是一样的，因此不再细究，有问题欢迎交流。</p>
<h1 id="5、代码实现"><a href="#5、代码实现" class="headerlink" title="5、代码实现"></a><strong>5、代码实现</strong></h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">public static void runCnn() &#123;</span><br><span class="line">    //创建一个卷积神经网络</span><br><span class="line">    LayerBuilder builder = new LayerBuilder();</span><br><span class="line">    builder.addLayer(Layer.buildInputLayer(new Size(28, 28)));</span><br><span class="line">    builder.addLayer(Layer.buildConvLayer(6, new Size(5, 5)));</span><br><span class="line">    builder.addLayer(Layer.buildSampLayer(new Size(2, 2)));</span><br><span class="line">    builder.addLayer(Layer.buildConvLayer(12, new Size(5, 5)));</span><br><span class="line">    builder.addLayer(Layer.buildSampLayer(new Size(2, 2)));</span><br><span class="line">    builder.addLayer(Layer.buildOutputLayer(10));</span><br><span class="line">    CNN cnn = new CNN(builder, 50);</span><br><span class="line">    </span><br><span class="line">    //导入数据集</span><br><span class="line">    String fileName = &quot;dataset/train.format&quot;;</span><br><span class="line">    Dataset dataset = Dataset.load(fileName, &quot;,&quot;, 784);</span><br><span class="line">    cnn.train(dataset, 3);//</span><br><span class="line">    String modelName = &quot;model/model.cnn&quot;;</span><br><span class="line">    cnn.saveModel(modelName);        </span><br><span class="line">    dataset.clear();</span><br><span class="line">    dataset = null;</span><br><span class="line">    </span><br><span class="line">    //预测</span><br><span class="line">    // CNN cnn = CNN.loadModel(modelName);    </span><br><span class="line">    Dataset testset = Dataset.load(&quot;dataset/test.format&quot;, &quot;,&quot;, -1);</span><br><span class="line">    cnn.predict(testset, &quot;dataset/test.predict&quot;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/03/16/180316_poi大数据读写excel/" rel="next" title="poi大数据读写excel">
                <i class="fa fa-chevron-left"></i> poi大数据读写excel
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/05/18/180518_如何高效学习/" rel="prev" title="如何高效学习">
                如何高效学习 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">zhuangzhouzhishui</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">74</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">22</span>
                  <span class="site-state-item-name">tags</span>
                
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#前言"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#1、CNN的整体网络结构"><span class="nav-number">2.</span> <span class="nav-text">1、CNN的整体网络结构</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2、网络初始化"><span class="nav-number">3.</span> <span class="nav-text">2、网络初始化</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3、前向传输计算"><span class="nav-number">4.</span> <span class="nav-text">3、前向传输计算</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4、反向传输调整权重"><span class="nav-number">5.</span> <span class="nav-text">4、反向传输调整权重</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#5、代码实现"><span class="nav-number">6.</span> <span class="nav-text">5、代码实现</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">zhuangzhouzhishui</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
