<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="机器学习,">










<meta name="description" content="1. GBDT + LR 是什么本质上GBDT+LR是一种具有stacking思想的二分类器模型，所以可以用来解决二分类问题。这个方法出自于Facebook 2014年的论文 Practical Lessons from Predicting Clicks on Ads at Facebook 。 2. GBDT + LR 用在哪GBDT+LR 使用最广泛的场景是CTR点击率预估，即预测当给用户推">
<meta name="keywords" content="机器学习">
<meta property="og:type" content="article">
<meta property="og:title" content="GBDT+LR算法解析及Python实现">
<meta property="og:url" content="http://JackArch.github.io/2018/06/05/180605_GBDT+LR算法解析及Python实现/index.html">
<meta property="og:site_name" content="Deep  |  Mind">
<meta property="og:description" content="1. GBDT + LR 是什么本质上GBDT+LR是一种具有stacking思想的二分类器模型，所以可以用来解决二分类问题。这个方法出自于Facebook 2014年的论文 Practical Lessons from Predicting Clicks on Ads at Facebook 。 2. GBDT + LR 用在哪GBDT+LR 使用最广泛的场景是CTR点击率预估，即预测当给用户推">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://jackarch.github.io/2018/06/05/180605_GBDT+LR算法解析及Python实现/pic/180601.jpg">
<meta property="og:image" content="http://jackarch.github.io/2018/06/05/180605_GBDT+LR算法解析及Python实现/pic/180602.jpg">
<meta property="og:image" content="http://jackarch.github.io/2018/06/05/180605_GBDT+LR算法解析及Python实现/pic/180603.jpg">
<meta property="og:image" content="http://jackarch.github.io/2018/06/05/180605_GBDT+LR算法解析及Python实现/pic/180604.jpg">
<meta property="og:updated_time" content="2019-06-19T12:06:51.536Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="GBDT+LR算法解析及Python实现">
<meta name="twitter:description" content="1. GBDT + LR 是什么本质上GBDT+LR是一种具有stacking思想的二分类器模型，所以可以用来解决二分类问题。这个方法出自于Facebook 2014年的论文 Practical Lessons from Predicting Clicks on Ads at Facebook 。 2. GBDT + LR 用在哪GBDT+LR 使用最广泛的场景是CTR点击率预估，即预测当给用户推">
<meta name="twitter:image" content="http://jackarch.github.io/2018/06/05/180605_GBDT+LR算法解析及Python实现/pic/180601.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://JackArch.github.io/2018/06/05/180605_GBDT+LR算法解析及Python实现/">





  <title>GBDT+LR算法解析及Python实现 | Deep  |  Mind</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Deep  |  Mind</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://JackArch.github.io/2018/06/05/180605_GBDT+LR算法解析及Python实现/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="zhuangzhouzhishui">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Deep  |  Mind">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">GBDT+LR算法解析及Python实现</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-05T17:53:12+08:00">
                2018-06-05
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="1-GBDT-LR-是什么"><a href="#1-GBDT-LR-是什么" class="headerlink" title="1. GBDT + LR 是什么"></a>1. GBDT + LR 是什么</h2><p>本质上GBDT+LR是一种具有stacking思想的二分类器模型，所以可以用来解决二分类问题。这个方法出自于Facebook 2014年的论文 <a href="https://pdfs.semanticscholar.org/daf9/ed5dc6c6bad5367d7fd8561527da30e9b8dd.pdf" target="_blank" rel="noopener">Practical Lessons from Predicting Clicks on Ads at Facebook</a> 。</p>
<h2 id="2-GBDT-LR-用在哪"><a href="#2-GBDT-LR-用在哪" class="headerlink" title="2. GBDT + LR 用在哪"></a>2. GBDT + LR 用在哪</h2><p>GBDT+LR 使用最广泛的场景是CTR点击率预估，即预测当给用户推送的广告会不会被用户点击。</p>
<p>点击率预估模型涉及的训练样本一般是上亿级别，样本量大，模型常采用速度较快的LR。但LR是线性模型，学习能力有限，此时特征工程尤其重要。现有的特征工程实验，主要集中在寻找到有区分度的特征、特征组合，折腾一圈未必会带来效果提升。GBDT算法的特点正好可以用来发掘有区分度的特征、特征组合，减少特征工程中人力成本。</p>
<p>从知乎<a href="https://zhuanlan.zhihu.com/p/29053940" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/29053940</a>上看到了一个关于CTR的流程，如下图所示：</p>
<h2 id><a href="#" class="headerlink" title></a><img src="./pic/180601.jpg" alt="img"></h2><p>如上图，主要包括两大部分：离线部分、在线部分，其中离线部分目标主要是训练出可用模型，而在线部分则考虑模型上线后，性能可能随时间而出现下降，弱出现这种情况，可选择使用Online-Learning来在线更新模型：</p>
<h4 id="2-1-离线部分"><a href="#2-1-离线部分" class="headerlink" title="2.1 离线部分"></a>2.1 离线部分</h4><ol>
<li>数据收集：主要收集和业务相关的数据，通常会有专门的同事在app位置进行埋点，拿到业务数据</li>
<li>预处理：对埋点拿到的业务数据进行去脏去重；</li>
<li>构造数据集：经过预处理的业务数据，构造数据集，在切分训练、测试、验证集时应该合理根据业务逻辑来进行切分；</li>
<li>特征工程：对原始数据进行基本的特征处理，包括去除相关性大的特征，离散变量one-hot，连续特征离散化等等;</li>
<li>模型选择：选择合理的机器学习模型来完成相应工作，原则是先从简入深，先找到baseline，然后逐步优化；</li>
<li>超参选择：利用gridsearch、randomsearch或者hyperopt来进行超参选择，选择在离线数据集中性能最好的超参组合；</li>
<li>在线A/B Test：选择优化过后的模型和原先模型（如baseline）进行A/B Test，若性能有提升则替换原先模型；</li>
</ol>
<h4 id="2-2-在线部分"><a href="#2-2-在线部分" class="headerlink" title="2.2 在线部分"></a>2.2 在线部分</h4><ol>
<li>Cache &amp; Logic：设定简单过滤规则，过滤异常数据；</li>
<li>模型更新：当Cache &amp; Logic 收集到合适大小数据时，对模型进行pretrain+finetuning，若在测试集上比原始模型性能高，则更新model server的模型参数；</li>
<li>Model Server：接受数据请求，返回预测结果；</li>
</ol>
<h2 id="3-GBDT-LR-的结构"><a href="#3-GBDT-LR-的结构" class="headerlink" title="3. GBDT + LR 的结构"></a>3. GBDT + LR 的结构</h2><p>正如它的名字一样，GBDT+LR 由两部分组成，其中GBDT用来对训练集提取特征作为新的训练输入数据，LR作为新训练输入数据的分类器。</p>
<p>具体来讲，有以下几个步骤：</p>
<p><strong>3.1</strong> GBDT首先对原始训练数据做训练，得到一个二分类器，当然这里也需要利用网格搜索寻找最佳参数组合。</p>
<p><strong>3.2</strong> 与通常做法不同的是，当GBDT训练好做预测的时候，输出的并不是最终的二分类概率值，而是要把模型中的每棵树计算得到的预测概率值所属的叶子结点位置记为1，这样，就构造出了新的训练数据。</p>
<p>举个例子，下图是一个GBDT+LR 模型结构，设GBDT有两个弱分类器，分别以蓝色和红色部分表示，其中蓝色弱分类器的叶子结点个数为3，红色弱分类器的叶子结点个数为2，并且蓝色弱分类器中对0-1 的预测结果落到了第二个叶子结点上，红色弱分类器中对0-1 的预测结果也落到了第二个叶子结点上。那么我们就记蓝色弱分类器的预测结果为[0 1 0]，红色弱分类器的预测结果为[0 1]，综合起来看，GBDT的输出为这些弱分类器的组合[0 1 0 0 1] ，或者一个稀疏向量（数组）。</p>
<p>这里的思想与One-hot独热编码类似，事实上，在用GBDT构造新的训练数据时，采用的也正是One-hot方法。并且由于每一弱分类器有且只有一个叶子节点输出预测结果，所以在一个具有n个弱分类器、共计m个叶子结点的GBDT中，每一条训练数据都会被转换为1*m维稀疏向量，且有n个元素为1，其余m-n 个元素全为0。</p>
<p><strong>3.3</strong> 新的训练数据构造完成后，下一步就要与原始的训练数据中的label(输出)数据一并输入到Logistic Regression分类器中进行最终分类器的训练。思考一下，在对原始数据进行GBDT提取为新的数据这一操作之后，数据不仅变得稀疏，而且由于弱分类器个数，叶子结点个数的影响，可能会导致新的训练数据特征维度过大的问题，因此，在Logistic Regression这一层中，可使用正则化来减少过拟合的风险，在Facebook的论文中采用的是L1正则化。</p>
<p><img src="./pic/180602.jpg" alt="img"></p>
<h2 id="4-RF-LR-Xgb-LR"><a href="#4-RF-LR-Xgb-LR" class="headerlink" title="4. RF + LR ? Xgb + LR?"></a>4. RF + LR ? Xgb + LR?</h2><p>有心的同学应该会思考一个问题，既然GBDT可以做新训练样本的构造，那么其它基于树的模型，例如Random Forest以及Xgboost等是并不是也可以按类似的方式来构造新的训练样本呢？没错，所有这些基于树的模型都可以和Logistic Regression分类器组合。至于效果孰优孰劣，我个人觉得效果都还可以，但是之间没有可比性，因为超参数的不同会对模型评估产生较大的影响。下图是RF+LR、GBT+LR、Xgb、LR、Xgb+LR 模型效果对比图，然而这只能做个参考，因为模型超参数的值的选择这一前提条件都各不相同。</p>
<p>顺便来讲，RF也是多棵树，但从效果上有实践证明不如GBDT。且GBDT前面的树，特征分裂主要体现对多数样本有区分度的特征；后面的树，主要体现的是经过前N颗树，残差仍然较大的少数样本。优先选用在整体上有区分度的特征，再选用针对少数样本有区分度的特征，思路更加合理，这应该也是用GBDT的原因。</p>
<p> <img src="./pic/180603.jpg" alt="img"></p>
<p> <img src="./pic/180604.jpg" alt="img"></p>
<h2 id="5-GBDT-LR-代码分析"><a href="#5-GBDT-LR-代码分析" class="headerlink" title="5. GBDT + LR 代码分析"></a>5. GBDT + LR 代码分析</h2><p>在网上找到了两个版本的GBDT+LR的代码实现，通过阅读分析，认为里面有一些细节还是值得好好学习一番的，所以接下来这一小节会针对代码实现部分做一些总结。</p>
<p>首先，目前我所了解到的GBDT的实现方式有两种：一是利用Scikit-learn中的ensemble.GradientBoostingClassifier ，二是利用lgb里的params={ ‘boosting_type’: ‘gbdt’ }参数。接下里分别对这两种实现方式进行分析。</p>
<h4 id="5-1-Scikit-learn的实现："><a href="#5-1-Scikit-learn的实现：" class="headerlink" title="5.1 Scikit-learn的实现："></a>5.1 Scikit-learn的实现：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.preprocessing import OneHotEncoder</span><br><span class="line">from sklearn.ensemble import GradientBoostingClassifier</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">gbm1 = GradientBoostingClassifier(n_estimators=50, random_state=10, subsample=0.6, max_depth=7,</span><br><span class="line">                                  min_samples_split=900)</span><br><span class="line">gbm1.fit(X_train, Y_train)</span><br><span class="line">train_new_feature = gbm1.apply(X_train)</span><br><span class="line">train_new_feature = train_new_feature.reshape(-1, 50)</span><br><span class="line"></span><br><span class="line">enc = OneHotEncoder()</span><br><span class="line"></span><br><span class="line">enc.fit(train_new_feature)</span><br><span class="line"></span><br><span class="line"># # 每一个属性的最大取值数目</span><br><span class="line"># print(&apos;每一个特征的最大取值数目:&apos;, enc.n_values_)</span><br><span class="line"># print(&apos;所有特征的取值数目总和:&apos;, enc.n_values_.sum())</span><br><span class="line"></span><br><span class="line">train_new_feature2 = np.array(enc.transform(train_new_feature).toarray())</span><br></pre></td></tr></table></figure>

<h4 id="划重点："><a href="#划重点：" class="headerlink" title="划重点："></a>划重点：</h4><h4 id="5-1-1-model-apply-X-train-的用法"><a href="#5-1-1-model-apply-X-train-的用法" class="headerlink" title="5.1.1 model.apply(X_train)的用法"></a>5.1.1 model.apply(X_train)的用法</h4><p>model.apply(X_train)返回训练数据X_train在训练好的模型里每棵树中所处的叶子节点的位置（索引）</p>
<h4 id="5-1-2-sklearn-preprocessing-中OneHotEncoder的使用"><a href="#5-1-2-sklearn-preprocessing-中OneHotEncoder的使用" class="headerlink" title="5.1.2 sklearn.preprocessing 中OneHotEncoder的使用"></a>5.1.2 sklearn.preprocessing 中OneHotEncoder的使用</h4><p>除了pandas中的 get_dummies()，sklearn也提供了一种对Dataframe做One-hot的方法。</p>
<p>OneHotEncoder() 首先fit() 过待转换的数据后，再次transform() 待转换的数据，就可实现对这些数据的所有特征进行One-hot 操作。</p>
<p>由于transform() 后的数据格式不能直接使用，所以最后需要使用.toarray() 将其转换为我们能够使用的数组结构。</p>
<blockquote>
<p>enc.transform(train_new_feature).toarray()</p>
</blockquote>
<h4 id="5-1-3-sklearn中的GBDT-能够设置树的个数，每棵树最大叶子节点个数等超参数，但不能指定每颗树的叶子节点数。"><a href="#5-1-3-sklearn中的GBDT-能够设置树的个数，每棵树最大叶子节点个数等超参数，但不能指定每颗树的叶子节点数。" class="headerlink" title="5.1.3 sklearn中的GBDT 能够设置树的个数，每棵树最大叶子节点个数等超参数，但不能指定每颗树的叶子节点数。"></a>5.1.3 sklearn中的GBDT 能够设置树的个数，每棵树最大叶子节点个数等超参数，但不能指定每颗树的叶子节点数。</h4><h4 id="5-2-lightgbm-的实现"><a href="#5-2-lightgbm-的实现" class="headerlink" title="5.2 lightgbm 的实现"></a>5.2 lightgbm 的实现</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">params = &#123;</span><br><span class="line">    &apos;task&apos;: &apos;train&apos;,</span><br><span class="line">    &apos;boosting_type&apos;: &apos;gbdt&apos;,</span><br><span class="line">    &apos;objective&apos;: &apos;binary&apos;,</span><br><span class="line">    &apos;metric&apos;: &#123;&apos;binary_logloss&apos;&#125;,</span><br><span class="line">    &apos;num_leaves&apos;: 64,</span><br><span class="line">    &apos;num_trees&apos;: 100,</span><br><span class="line">    &apos;learning_rate&apos;: 0.01,</span><br><span class="line">    &apos;feature_fraction&apos;: 0.9,</span><br><span class="line">    &apos;bagging_fraction&apos;: 0.8,</span><br><span class="line">    &apos;bagging_freq&apos;: 5,</span><br><span class="line">    &apos;verbose&apos;: 0</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># number of leaves,will be used in feature transformation</span><br><span class="line">num_leaf = 64</span><br><span class="line"></span><br><span class="line">print(&apos;Start training...&apos;)</span><br><span class="line"># train</span><br><span class="line">gbm = lgb.train(params=params,</span><br><span class="line">                train_set=lgb_train,</span><br><span class="line">                valid_sets=lgb_train, )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(&apos;Start predicting...&apos;)</span><br><span class="line"># y_pred分别落在100棵树上的哪个节点上</span><br><span class="line">y_pred = gbm.predict(x_train, pred_leaf=True)</span><br><span class="line">y_pred_prob = gbm.predict(x_train)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">result = []</span><br><span class="line">threshold = 0.5</span><br><span class="line">for pred in y_pred_prob:</span><br><span class="line">    result.append(1 if pred &gt; threshold else 0)</span><br><span class="line">print(&apos;result:&apos;, result)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(&apos;Writing transformed training data&apos;)</span><br><span class="line">transformed_training_matrix = np.zeros([len(y_pred), len(y_pred[1]) * num_leaf],</span><br><span class="line">                                       dtype=np.int64)  # N * num_tress * num_leafs</span><br><span class="line">for i in range(0, len(y_pred)):</span><br><span class="line">    # temp表示在每棵树上预测的值所在节点的序号（0,64,128,...,6436 为100棵树的序号，中间的值为对应树的节点序号）</span><br><span class="line">    temp = np.arange(len(y_pred[0])) * num_leaf + np.array(y_pred[i])</span><br><span class="line">    # 构造one-hot 训练数据集</span><br><span class="line">    transformed_training_matrix[i][temp] += 1</span><br><span class="line"></span><br><span class="line">y_pred = gbm.predict(x_test, pred_leaf=True)</span><br><span class="line">print(&apos;Writing transformed testing data&apos;)</span><br><span class="line">transformed_testing_matrix = np.zeros([len(y_pred), len(y_pred[1]) * num_leaf], dtype=np.int64)</span><br><span class="line">for i in range(0, len(y_pred)):</span><br><span class="line">    temp = np.arange(len(y_pred[0])) * num_leaf + np.array(y_pred[i])</span><br><span class="line">    # 构造one-hot 测试数据集</span><br><span class="line">    transformed_testing_matrix[i][temp] += 1</span><br></pre></td></tr></table></figure>

<h4 id="划重点：-1"><a href="#划重点：-1" class="headerlink" title="划重点："></a><strong>划重点：</strong></h4><h4 id="5-2-1-params-字典里超参数的设置"><a href="#5-2-1-params-字典里超参数的设置" class="headerlink" title="5.2.1 params 字典里超参数的设置"></a>5.2.1 params 字典里超参数的设置</h4><p>因为是二分类问题，所以设置 {‘boosting_type’: ‘gbdt’,’objective’: ‘binary’,’metric’: {‘binary_logloss’}}，然后设置树的个数及每棵树的叶子结点个数{‘num_leaves’: 64,’num_trees’: 100}</p>
<h4 id="5-2-2-model-predict-x-train-pred-leaf-True"><a href="#5-2-2-model-predict-x-train-pred-leaf-True" class="headerlink" title="5.2.2 model.predict(x_train, pred_leaf=True)"></a>5.2.2 model.predict(x_train, pred_leaf=True)</h4><p>使用</p>
<blockquote>
<p>model.predict(x_train, pred_leaf=True) </p>
</blockquote>
<p>返回训练数据在训练好的模型里预测结果所在的每棵树中叶子节点的位置（索引），形式为7999*100的二维数组。</p>
<h4 id="5-2-3-构造Ont-hot数组作为新的训练数据"><a href="#5-2-3-构造Ont-hot数组作为新的训练数据" class="headerlink" title="5.2.3 构造Ont-hot数组作为新的训练数据"></a>5.2.3 构造Ont-hot数组作为新的训练数据</h4><p>这里并没有使用sklearn中的OneHotEncoder()，也没有使用pandas中的get_dummies()，而是手工创建一个One-hot数组。(当然也可以像5.1.2 那样操作)</p>
<ol>
<li>首先，创建一个二维零数组用于存放one-hot的元素；</li>
<li>然后，获取第2步得到的二维数组里每个叶子节点在整个GBDT模型里的索引号，因为一共有100棵树，每棵树有64个叶子节点，所以索引范围是0~6400；（这里有一个技巧，通过把每棵树的起点索引组成一个列表，再加上由落在每棵树叶子节点的索引组成的列表，就得到了往二维零数组里插入元素的索引信息）</li>
<li>最后，</li>
</ol>
<blockquote>
<p>temp = np.arange(len(y_pred[0])) * num_leaf + np.array(y_pred[i])</p>
</blockquote>
<h4 id="5-2-4-对二维数组填充信息，采用”-”-的方法"><a href="#5-2-4-对二维数组填充信息，采用”-”-的方法" class="headerlink" title="5.2.4 对二维数组填充信息，采用”+=” 的方法"></a>5.2.4 对二维数组填充信息，采用”+=” 的方法</h4><p># 构造one-hot 训练数据集</p>
<blockquote>
<p>transformed_training_matrix[i][temp] += 1</p>
</blockquote>
<h2 id="6-GBDT-LR-模型提升"><a href="#6-GBDT-LR-模型提升" class="headerlink" title="6. GBDT + LR 模型提升"></a><strong>6. GBDT + LR 模型提升</strong></h2><p>现在，我们思考这样一个问题，Logistic Regression是一个线性分类器，也就是说会忽略掉特征与特征之间的关联信息，那么是否可以采用构建新的交叉特征这一特征组合方式从而提高模型的效果？</p>
<p>其次，我们已经在2.3小节中了解到GBDT很有可能构造出的新训练数据是高维的稀疏矩阵，而Logistic Regression使用高维稀疏矩阵进行训练，会直接导致计算量过大，特征权值更新缓慢的问题。</p>
<p>针对上面可能出现的问题，可以翻看我之前的文章：<a href="https://www.cnblogs.com/wkang/p/9588360.html" target="_blank" rel="noopener">FM算法解析及Python实现</a> ，使用FM算法代替LR，这样就解决了Logistic Regression的模型表达效果及高维稀疏矩阵的训练开销较大的问题。然而，这样就意味着可以高枕无忧了吗？当然不是，因为采用FM对本来已经是高维稀疏矩阵做完特征交叉后，新的特征维度会更加多，并且由于元素非0即1，新的特征数据可能也会更加稀疏，那么怎么办？</p>
<p>所以，我们需要再次回到GBDT构造新训练数据这里。当GBDT构造完新的训练样本后，我们要做的是对每一个特征做与输出之间的特征重要度评估并筛选出重要程度较高的部分特征，这样，GBDT构造的高维的稀疏矩阵就会减少一部分特征，也就是说得到的稀疏矩阵不再那么高维了。之后，对这些筛选后得到的重要度较高的特征再做FM算法构造交叉项，进而引入非线性特征，继而完成最终分类器的训练数据的构造及模型的训练。</p>
<h2 id="7-参考资料"><a href="#7-参考资料" class="headerlink" title="7. 参考资料"></a>7. 参考资料</h2><p>[1] <a href="https://blog.csdn.net/lilyth_lilyth/article/details/48032119" target="_blank" rel="noopener">CTR预估中GBDT与LR融合方案</a></p>
<p>[2] <a href="https://zhuanlan.zhihu.com/p/29053940" target="_blank" rel="noopener">常见计算广告点击率预估算法总结</a></p>
<p>[3] <a href="https://blog.csdn.net/twt520ly/article/details/79769705" target="_blank" rel="noopener">GBDT+LR算法进行特征扩增</a></p>
<p>[4] <a href="https://www.jianshu.com/p/96173f2c2fb4" target="_blank" rel="noopener">推荐系统遇上深度学习(十)–GBDT+LR融合方案实战</a></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/机器学习/" rel="tag"># 机器学习</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/05/18/180518_如何高效学习/" rel="next" title="如何高效学习">
                <i class="fa fa-chevron-left"></i> 如何高效学习
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/07/20/180720_使用sklearn做特征工程/" rel="prev" title="使用sklearn做特征工程">
                使用sklearn做特征工程 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">zhuangzhouzhishui</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">74</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">22</span>
                  <span class="site-state-item-name">tags</span>
                
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-GBDT-LR-是什么"><span class="nav-number">1.</span> <span class="nav-text">1. GBDT + LR 是什么</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-GBDT-LR-用在哪"><span class="nav-number">2.</span> <span class="nav-text">2. GBDT + LR 用在哪</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#null"><span class="nav-number">3.</span> <span class="nav-text"></span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-离线部分"><span class="nav-number">3.0.1.</span> <span class="nav-text">2.1 离线部分</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-在线部分"><span class="nav-number">3.0.2.</span> <span class="nav-text">2.2 在线部分</span></a></li></ol></li></ol><li class="nav-item nav-level-2"><a class="nav-link" href="#3-GBDT-LR-的结构"><span class="nav-number">4.</span> <span class="nav-text">3. GBDT + LR 的结构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-RF-LR-Xgb-LR"><span class="nav-number">5.</span> <span class="nav-text">4. RF + LR ? Xgb + LR?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-GBDT-LR-代码分析"><span class="nav-number">6.</span> <span class="nav-text">5. GBDT + LR 代码分析</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-1-Scikit-learn的实现："><span class="nav-number">6.0.1.</span> <span class="nav-text">5.1 Scikit-learn的实现：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#划重点："><span class="nav-number">6.0.2.</span> <span class="nav-text">划重点：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-1-1-model-apply-X-train-的用法"><span class="nav-number">6.0.3.</span> <span class="nav-text">5.1.1 model.apply(X_train)的用法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-1-2-sklearn-preprocessing-中OneHotEncoder的使用"><span class="nav-number">6.0.4.</span> <span class="nav-text">5.1.2 sklearn.preprocessing 中OneHotEncoder的使用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-1-3-sklearn中的GBDT-能够设置树的个数，每棵树最大叶子节点个数等超参数，但不能指定每颗树的叶子节点数。"><span class="nav-number">6.0.5.</span> <span class="nav-text">5.1.3 sklearn中的GBDT 能够设置树的个数，每棵树最大叶子节点个数等超参数，但不能指定每颗树的叶子节点数。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-2-lightgbm-的实现"><span class="nav-number">6.0.6.</span> <span class="nav-text">5.2 lightgbm 的实现</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#划重点：-1"><span class="nav-number">6.0.7.</span> <span class="nav-text">划重点：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-2-1-params-字典里超参数的设置"><span class="nav-number">6.0.8.</span> <span class="nav-text">5.2.1 params 字典里超参数的设置</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-2-2-model-predict-x-train-pred-leaf-True"><span class="nav-number">6.0.9.</span> <span class="nav-text">5.2.2 model.predict(x_train, pred_leaf=True)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-2-3-构造Ont-hot数组作为新的训练数据"><span class="nav-number">6.0.10.</span> <span class="nav-text">5.2.3 构造Ont-hot数组作为新的训练数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-2-4-对二维数组填充信息，采用”-”-的方法"><span class="nav-number">6.0.11.</span> <span class="nav-text">5.2.4 对二维数组填充信息，采用”+=” 的方法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-GBDT-LR-模型提升"><span class="nav-number">7.</span> <span class="nav-text">6. GBDT + LR 模型提升</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-参考资料"><span class="nav-number">8.</span> <span class="nav-text">7. 参考资料</span></a></li></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">zhuangzhouzhishui</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
